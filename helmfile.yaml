repositories:
  - name: stable
    url: https://charts.helm.sh/stable
  - name: prometheus-community
    url: https://prometheus-community.github.io/helm-charts
  - name: ingress-nginx
    url: https://kubernetes.github.io/ingress-nginx
  - name: kiwigrid
    url: https://kiwigrid.github.io
  - name: jetstack
    url: https://charts.jetstack.io
  - name: loki
    url: https://grafana.github.io/helm-charts
  - name: itzg
    url: https://itzg.github.io/minecraft-server-charts/
  - name: k8s-at-home
    url: https://k8s-at-home.com/charts/
  - name: bitnami
    url: https://charts.bitnami.com/bitnami
  - name: presslabs
    url: https://presslabs.github.io/charts
  - name: descheduler
    url: https://kubernetes-sigs.github.io/descheduler/
  - name: harbor
    url: https://helm.goharbor.io
  - name: vector
    url: https://helm.vector.dev

releases:
  - name: cert-manager
    chart: jetstack/cert-manager
    version: ~1.5.0
    namespace: cert-manager
  - name: harbor
    chart: harbor/harbor
    version: ~1.13.0
    namespace: harbor
    values:
      - expose:
          type: ingress
          ingress:
            hosts:
              core: core.harbor.staplerstation.com
              notary: notary.harbor.staplerstation.com
            annotations:
              certmanager.k8s.io/cluster-issuer: letsencrypt-prod
              cert-manager.io/cluster-issuer: letsencrypt-prod
              kubernetes.io/ingress.class: nginx                                                                                                                                                                                                                                                                                                                                                                  │
              kubernetes.io/ssl-redirect: 'true'
          tls:
            certSource: secret
            secret:
              secretName: harbor-harbor-ingress-cert
              notarySecretName: harbor-harbor-notary-ingress-cert
        database:
          internal:
            image:
              tag: v2.2.2
        metrics:
          enabled: true
          serviceMonitor:
            enabled: true
        externalURL: https://core.harbor.staplerstation.com
      - values/harbor_values.yaml
  - name: descheduler
    chart: descheduler/descheduler
    version: ~0.28.0
    namespace: kube-system
  - name: goldpinger
    chart: stable/goldpinger
    version: ~2.0.1
    values:
      - resources:
          limits:
            memory: 50Mi
          requests:
            memory: 50Mi
        serviceMonitor:
          enabled: true
          namespace: default
  - name: nginx-ingress
    chart: ingress-nginx/ingress-nginx
    version: ~4.8
    values:
      - controller:
          ingressClassResource:
            default: true
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          nodeSelector:
            kubernetes.io/hostname: nimbus
          minAvailable: 0
          tolerations:
            - key: cloud
              operator: Exists
              effect: NoSchedule
          resources:
           limits:
             memory: 400Mi
           requests:
             cpu: 100m
             memory: 400Mi
          admissionWebhooks:
            enabled: false
        defaultBackend:
          minAvailable: 0
          resources:
            limits:
              cpu: 10m
              memory: 20Mi
            requests:
              cpu: 10m
              memory: 20Mi
        rbac:
          create: true
  - name: home-assistant
    chart: k8s-at-home/home-assistant
    version: ~13.4.0
    values:
      - persistence:
          config:
            enabled: true
            size: 8Gi
            retain: true
        probes:
          liveness:
            enabled: false
        http:
          server_host: 0.0.0.0
          ip_ban_enabled: true
          login_attempts_threshold: 5
          use_x_forwarded_for: true
          trusted_proxies:
          - 10.0.0.6
          # Pod CIDR
          - 10.233.64.0/18
          # Node CIDR
          - 10.0.0.0/24
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                  - key: "kubernetes.io/hostname"
                    operator: In
                    values: ["absis", "leviathan", "smicro1"]
        hostNetwork: true
        metrics:
          enables: true
        monitoring:
          enabled: true
        resources:
          limits:
           memory: 2Gi
          requests:
           cpu: 500m
           memory: 2Gi
      - values/home-assistant_values.yaml
  - name: transmission-ovpn
    chart: ./transmission-ovpn
    version: ~1.0
    values:
      - resources:
          requests:
            cpu: .25
            memory: 5Gi
          limits:
            memory: 5Gi
      - values/transmission-ovpn.yaml
  - name: radarr
    chart: ./radarr
    version: ~1.0
    values:
      - values/radarr.yaml
  - name: sonarr
    chart: ./sonarr
    version: ~1.0
    values:
      - values/sonarr.yaml
  - name: prowlarr
    chart: ./prowlarr
    version: ~0.1.0
  - name: jackett
    chart: ./jackett
    version: ~1.0
    values:
      - values/jackett.yaml
  - name: flaresolverr
    chart: ./flaresolverr
    version: ~0.1.0
  - name: wobbling-crab
    chart: ./ombi
    version: ~0.1.0
    values:
      - values/ombi_values.yaml
  - name: logseq
    chart: ./logseq
    version: ~0.1.0
    values:
      - resources:
          requests:
            memory: 250Mi
          limits:
            memory: 250Mi
      - values/logseq_values.yaml
  - name: solemn-echidna
    chart: ./plex
    version: ~0.3.1
    values:
      - values/plex_values.yaml
  - name: vector
    chart: vector/vector
    version: ~0.26.0
    namespace: vector
  - name: prometheus
    chart: prometheus-community/kube-prometheus-stack
    version: ^35.0.3
    values:
      - alertmanager:
          alertmanagerSpec:
            resources:
              requests:
                memory: 400Mi
              limits:
                memory: 400Mi
            storageSpec:
              volumeClaimTemplate:
               spec:
                 storageClassName: cephfs
                 accessModes: ["ReadWriteMany"]
                 resources:
                   requests:
                     storage: 5Gi
        prometheusOperator:
          resources:
            limits:
              cpu: 200m
              memory: 400Mi
            requests:
              cpu: 100m
              memory: 200Mi
        prometheus:
          prometheusSpec:
            securityContext:
              runAsNonRoot: false
              runAsUser: 0
              fsGroup: 0
            retention: 12w
            storageSpec:
              volumeClaimTemplate:
               spec:
                 storageClassName: cephfs
                 accessModes: ["ReadWriteMany"]
                 resources:
                   requests:
                     storage: 100Gi
            serviceMonitorSelectorNilUsesHelmValues: false
            additionalScrapeConfigs:
            - job_name: 'ceph'
              honor_labels: true
              static_configs:
              - targets:
                - "10.0.0.1:9283"
                - "10.0.0.3:9283"
                - "10.0.0.4:9283"
                - "10.0.0.6:9283"
                labels: { "cluster": "stapler1" }
            resources:
              requests:
                memory: 5Gi
              limits:
                memory: 5Gi
        prometheus-node-exporter:
          resources:
            limits:
              cpu: 200m
              memory: 80Mi
            requests:
              cpu: 100m
              memory: 50Mi
        grafana:
          enabled: true
          defaultDashboardsEnabled: false
          ingress:
            enabled: true
            annotations:
              cert-manager.io/cluster-issuer: letsencrypt-prod
              kubernetes.io/ingress.class: nginx                                                                                                                                                                                                                                                                                                                                                                  │
              kubernetes.io/ssl-redirect: 'true'
            hosts:
              - graph.staplerstation.com
            tls:
            - hosts:
                - graph.staplerstation.com
              secretName: grafana-ingress
          grafana.ini:
            server:
              root_url: https://graph.staplerstation.com:443
              domain: graph.staplerstation.com
          resources:
            limits:
              memory: 1Gi
            requests:
              memory: 1Gi
          persistence:
            enabled: true
            size: 32Gi
            accessMode: ReadWriteMany
            storageClassName: "cephfs"
          extraContainers: |
            - name: renderer
              image: grafana/grafana-image-renderer:latest
          env:
            GF_RENDERING_SERVER_URL: http://localhost:8081/render
            GF_RENDERING_CALLBACK_URL: http://localhost:3000/
  - name: loki
    chart: loki/loki-stack
    version: 2.0.2
    values:
      - loki:
          resources:
            limits:
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 512Mi
  - name: unifi-exporter
    chart: ./unifi-exporter
    version: ~0.1.0
    values:
      - config:
          listen:
            address: :9130
            metricspath: /metrics
      - values/unifi_exporter.yaml
  - name: zoneminder
    chart: ./zoneminder
    version: ~0.1.0
    values:
      - resources:
          requests:
            memory: 2Gi
            cpu: 2
          limits:
            memory: 2Gi
        service:
          type: LoadBalancer
      - values/zoneminder_values.yaml
  - name: metallb
    chart: stable/metallb
    version: ~0.12.0
    values:
    - configInline:
        address-pools:
        - name: default
          protocol: layer2
          addresses:
          - 192.168.1.220-192.168.1.240
      controller:
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
      speaker:
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                  - key: "kubernetes.io/hostname"
                    operator: In
                    values: ["absis", "leviathan", "smicro1"]
